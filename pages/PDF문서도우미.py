from langchain_community.document_loaders import PyPDFLoader #1
from langchain_text_splitters import RecursiveCharacterTextSplitter #2
from langchain_google_genai import GoogleGenerativeAIEmbeddings #3
from langchain_community.vectorstores import FAISS #4
from langchain_community.vectorstores.utils import DistanceStrategy #4
from langchain_core.runnables import RunnablePassthrough #5
from langchain_core.prompts import PromptTemplate #6
from langchain_google_genai import ChatGoogleGenerativeAI #7
from langchain_core.output_parsers import StrOutputParser #7
import streamlit as st
from dotenv import load_dotenv
import os
load_dotenv()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

#í™”ë©´êµ¬ì„±
st.title('PDFë¬¸ì„œ ì‘ë‹µğŸ“œ')
st.info('''
        ##### ì±—ë´‡ì†Œê°œğŸ’¬   
        ###### ì—…ë¡œë“œí•œ :green[pdfíŒŒì¼]ì— ëŒ€í•˜ì—¬ ì§ˆì˜í•˜ë©´ :blue[ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì°¸ê³ í•´ì„œ] ë‹µë³€í•©ë‹ˆë‹¤.
        ''')

'---'
# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
'---'
st.write('#### ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”')
question = st.text_input('',placeholder='pdfë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ ì…ë ¥')

#step1 ë¬¸ì„œë¡œë“œ
if uploaded_file is not None:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getbuffer())  # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥

    loader = PyPDFLoader("temp.pdf")
    pages = loader.load()

    #step2 ë¬¸ì„œë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400, #(['\n\n', '\n', ' ', ''])ë¬¸ìë¥¼ ìˆœì„œëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼  chunk_sizeë³´ë‹¤ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ ë¶„í• 
        chunk_overlap=50, #ë¶„í• ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ ì‚¬ì´ì—ì„œ ì¤‘ë³µìœ¼ë¡œ í¬í•¨ë  ë¬¸ì ìˆ˜
        length_function = len
    )
    split_documents = text_splitter.split_documents(pages)

    #step3 ì„ë² ë”©
    embeddings_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001')

    

    #step4 ë²¡í„° ìŠ¤í† ì–´
    vectorstore = FAISS.from_documents(
        documents=split_documents, 
        embedding=embeddings_model,
        distance_strategy = DistanceStrategy.COSINE #ì„¤ì •ì„ ì•ˆí•˜ë©´ ê¸°ë³¸ì€ ìœ í´ë¦¬ë“œ ìœ ì‚¬ë„ ê²€ì‚¬ ì‚¬ìš©
    )

    #step5 ê²€ìƒ‰ê¸°
    retriever = vectorstore.as_retriever()

#step6 í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Answer in Korean.

#Context: 
{context} 

#Question: 
{question} 

#Answer:"""
)

if 'retriever' in locals() and question != '' :
    #step7 ì–¸ì–´ëª¨ë¸ ìƒì„±
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2
    )
    chain = prompt | llm | StrOutputParser

    #step8 ì²´ì¸ìƒì„±
    chain = (
        {"context":retriever, "question":RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    with st.spinner('ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
        answer = chain.invoke(question)
        st.error(answer)


